Risk Level Prediction using Machine Learning
ğŸ“Œ Overview

This project presents a complete end-to-end machine learning pipeline for multi-class risk level prediction using structured (tabular) clinical data.
It demonstrates real-world ML practices including data cleaning, class imbalance handling, feature importance analysis, dimensionality reduction, and comparative evaluation of multiple machine learning models.

The project is designed for learning, experimentation, and portfolio demonstration.

ğŸ¯ Problem Statement

Given a dataset containing multiple clinical features, the objective is to predict the risk level of each instance into one of the following categories:

Low Risk (0)
Mid Risk (1)
High Risk (2)

ğŸ› ï¸ Technologies & Libraries

Python
Pandas
NumPy
Scikit-learn
Imbalanced-learn (SMOTE)
XGBoost
Matplotlib

Seaborn

ğŸ“‚ Project Structure
Risk-Prediction-with-PCA/
â”‚
â”œâ”€ data.csv                 # Tabular dataset
â”œâ”€ Case1.py                 # Feature Importance: Random Forest, Prediction: Logistic Regression
â”œâ”€ Case2.py                 # Feature Importance: XGBoost, Prediction: Random Forest
â”œâ”€ Case3.py                 # PCA-based Feature Extraction; Models: RF, XGBoost, Logistic Regression; Visualization
â””â”€ README.md                # Project documentation

ğŸ§© Workflow

All three cases follow the same core pipeline:
Data Loading & Preprocessing
Load data.csv
Remove duplicates
Remove age group 10â€“18
Map risk levels to numeric (0 = low, 1 = mid, 2 = high)
Train/Test Split
80% training / 20% testing
Stratified sampling to maintain class distribution
Class Imbalance Handling
Apply SMOTE on training data

Feature Importance / Dimensionality Reduction
Case 1: Feature importance via Random Forest
Case 2: Feature importance via XGBoost
Case 3: PCA (3 components)

Model Training & Prediction
Case 1: Logistic Regression
Case 2: Random Forest
Case 3: Random Forest, XGBoost, Logistic Regression (on PCA-transformed features)

Evaluation Metrics
Accuracy
Confusion matrix
Classification report

Visualizations (Case 3)
Rows remaining after each preprocessing step
Class distribution before and after SMOTE
Confusion matrix heatmap
Feature importance of PCA components
Precision, Recall, F1-score per class

ğŸš€ Getting Started
1ï¸âƒ£ Install Dependencies
pip install pandas numpy scikit-learn imbalanced-learn xgboost matplotlib seaborn

2ï¸âƒ£ Run Scripts
Case 1: python Case1.py
Case 2: python Case2.py
Case 3: python Case3.py
Ensure data.csv is in the same folder as the scripts.

ğŸ“Š Results Overview

Case 1: Feature importance using Random Forest; Logistic Regression predictions.
Case 2: Feature importance using XGBoost; Random Forest predictions.
Case 3: PCA-based feature extraction; multiple classifier training; detailed visualizations.

ğŸ“ Conclusion
This repository demonstrates a robust machine learning workflow for multi-class risk prediction:
Data preprocessing and cleaning
Handling class imbalance with SMOTE
Feature importance analysis and dimensionality reduction
Training and evaluating multiple classifiers
Clear visualizations for insights

It can be extended to other clinical datasets or multi-class classification problems
